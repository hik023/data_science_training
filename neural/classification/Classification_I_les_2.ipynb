{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nj-KrdIy7qn4"
   },
   "source": [
    "# Урок 2. Наивный байесовский классификатор.\n",
    "\n",
    "Наивный байесовский классификатор - семейство алгоритмов классификации, которые принимают допущение о том, что каждый параметр классифицируемых данных не зависит от других параметров объектов, т.е. ни один из параметров не оказывает влияние на другой.\n",
    "Согласитесь, что достаточно наивно было бы предполагать, что, допустим, рост и вес человека - совершенно независимые параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rx8vaDR7qn6"
   },
   "source": [
    "Для начала посмотрим на примере, как работает этот алгоритм.\n",
    "Допустим, у нас имеется небольшой набор данных Fruits, в котором представлена информация о видах {banana, orange, plum}. Отметим, что это просто конкретные имеющиеся \"измерения\", на которых обучается модель, и мы хотим научиться определять по этим данным, какой фрукт перед нами.\n",
    "\n",
    "В последнем столбце $Total$ представлено количество фруктов определенного класса (500 бананов, 300 апельсинов и 200 слив - всего 1000). В строке же $Total$ - общее количество фруктов с определенными признаками {long, sweet, yellow}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EWzRdMzm7qn9",
    "outputId": "73b06071-6c84-4bb4-a635-c5e6fe74b6a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Long</th>\n",
       "      <th>Sweet</th>\n",
       "      <th>Yellow</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Banana</th>\n",
       "      <td>400</td>\n",
       "      <td>350</td>\n",
       "      <td>450</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orange</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plum</th>\n",
       "      <td>30</td>\n",
       "      <td>180</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>430</td>\n",
       "      <td>680</td>\n",
       "      <td>850</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Long  Sweet  Yellow  Total\n",
       "Banana   400    350     450    500\n",
       "Orange     0    150     300    300\n",
       "Plum      30    180     100    200\n",
       "Total    430    680     850   1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = np.array([[400, 350, 450, 500],\n",
    "                 [0, 150, 300, 300],\n",
    "                 [30, 180, 100, 200],\n",
    "                 [430, 680, 850, 1000]])\n",
    "idx = ['Banana', 'Orange', 'Plum', 'Total']\n",
    "col = ['Long', 'Sweet', 'Yellow', 'Total']\n",
    "\n",
    "fruits = pd.DataFrame(data, columns=col, index=idx)\n",
    "fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo1MNpiFj21T"
   },
   "source": [
    "Здесь для простоты мы оперируем интуитивно понятными бинарными признаками, т.е. фактически любой из признаков мы можем определить как \"да\" или \"нет\". В соответствии с этим, например, получается, что среди бананов 400 из 500 длинные, 350 - сладкие, а 450 - желтые. Соответственно, если у нас \"на руках\" именно эти данные, то можно сказать, что, если у нас в руках банан, то он с вероятностью 80% длинный (вероятность считаем как отношение объектов с данным признаком к общему числу объектов -в данном случае 400/500 = 0,8), с вероятностью 70% (350/500 = 0,7) - сладкий, и с вероятностью 90% (450/500 = 0,9) - желтый.\n",
    "\n",
    "В случае с дискретными признаками (которые принимают значения исключительно из определенного набора значений) всегда составляется подобная частотная таблица для каждого класса и каждого признака, указывающая, сколько раз значение определенного признака встречается в каждом классе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7oBkxQS7qoB"
   },
   "source": [
    "Теперь, если мы получим только параметры - длину, сладость и цвет фрукта, сможем вычислить вероятность того, что фрукт является бананом, апельсином или сливой. Допустим, мы хотим узнать, к какому классу относится фрукт с параметрами {длинный, сладкий, желтый}. Вероятность того, что объект принадлежит какому-либо классу при данных параметрах обозначается как:\n",
    "\n",
    "$$\n",
    "P(Class|Long, Sweet, Yellow)\n",
    "$$\n",
    "\n",
    "Чтобы вычислить вероятность в случае дискретных (конечных) данных, мы делим количество соответствующих признаку объектов на общее число объектов. Например, если мы хотим узнать вероятность $P(Long|Banana)$, мы вычисляем $400 / 500 = 08$, как уже упоминалось выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXupWBYF7qn5"
   },
   "source": [
    "Данный метод основан на теореме английского математика-статистика Томаса Байеса. По сути, она позволяет предсказать класс на основании набора параметров, используя вероятность. Общая формула Байеса для класса с одним признаком выглядит так:\n",
    "\n",
    "$$\n",
    "P(Class A|Feature 1) = \\frac{P(Feature 1|Class A)\\cdot P(Class A)}{P(Feature 1)}\n",
    "$$\n",
    "\n",
    "$P(Class A|Feature 1)$ - вероятность того, что объект является классом $A$ при том, что его признак соответствует $Feature 1$.\n",
    "\n",
    "Упрощенное уравнение для классификации при двух признаках выглядит так:\n",
    "\n",
    "$$\n",
    "P(Class A|Feature 1, Feature 2) = \\frac{P(Feature 1|Class A)\\cdot P(Feature 2|Class A)\\cdot P(Class A)}{P(Feature 1)\\cdot P(Feature 2)}\n",
    "$$\n",
    "\n",
    "И далее для большего количества признаков формула меняется соответствующим образом.\n",
    "\n",
    "Напомним, что вероятность - число, которое принимает значения от 0 до 1, при этом 0 - полное несоответствие класса признакам, а 1 - однозначно определенный класс. Соответственно, чем ближе значение вероятности определенного класса к 1, тем больше шанс того, что объект принадлежит именно этому классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BORyqDxi7qn5"
   },
   "source": [
    "Знаменатель можно проигнорировать, так как он будет одинаков для всех вычислений и никакой важной информации не даст.\n",
    "Тогда получится следующее уравнение:\n",
    "\n",
    "$$\n",
    "P(Class A|Feature 1, Feature 2) = P(Feature 1|Class A)\\cdot P(Feature 2|Class A)\\cdot P(Class A)\n",
    "$$\n",
    "\n",
    "То есть для каждого возможного класса вычисляем только произведение вероятностей того, что каждый признак соответствует классу, и вероятности того, что объект принадлежит этому классу. Соответственно, наибольшее значение этого произведения, рассчитанного с признаками конкретного объекта, для какого-то из классов будет указывать на принадлежность объекта к этому классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUTX8J56j21V"
   },
   "source": [
    "Вернемся к нашему примеру с фруктами. Чтобы понять, к какому классу принадлежит объект с признаками {Long, Sweet, Yellow}, рассчитаем для каждого из классов формулу Байеса, используя для этого данные частотной таблицы. На выходе получаем вероятность того, что объект принадлежит классу. Класс с наибольшей вероятностью является ответом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QRQS33-Q7qoC",
    "outputId": "10161c2a-bba0-45d6-c967-5ac19e9b5059"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Banana': 0.252, 'Orange': 0.0, 'Plum': 0.013500000000000002}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {}\n",
    "for i in range (fruits.values.shape[0] - 1):\n",
    "    p = 1\n",
    "    for j in range (fruits.values.shape[1] - 1):\n",
    "        p *= fruits.values[i, j] / fruits.values[i, -1]\n",
    "    p *= fruits.values[i, -1] / fruits.values[-1, -1]\n",
    "    result[fruits.index[i]] = p\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wIhJJlN7qoG"
   },
   "source": [
    "Как мы видим, фрукт с параметрами {long, sweet, yellow} с наибольшей вероятностью принадлежит классу \"Banana\", что, кажется, соответствует реальности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRQHiisP7qoH"
   },
   "source": [
    "Но что делать, если у нас не частотная таблица, как в примере выше, а непрерывные данные? Например, мы не можем вычислять, сколько человек с конкретным ростом 1,81м, 1,67м и т.д. присутствует в выборке - нам это попросту ничего не даст, а лишь добавит громоздких вычислений. Поэтому обычно при непрерывных значениях параметров используется гауссовский наивный Байес, в котором сделано предположение о том, что значения параметров взяты из нормального распределения.\n",
    "\n",
    "![](https://248006.selcdn.ru/public/DS_Block2_M5_final/GNB.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX9_Z0Aw7qoJ"
   },
   "source": [
    "На графике - плотность вероятности нормального распределения. По сути, где больше площадь под графиком, там и наиболее вероятные значения. Поскольку способ представления значений в наборе данных изменяется, то и формула условной вероятности изменяется на\n",
    "\n",
    "$$\n",
    "p(x_i|y) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_y}}exp(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y})\n",
    "$$\n",
    "\n",
    "Здесь $\\sigma^2$ - дисперсия (разброс) данных, а $\\mu$ - математическое ожидание (среднее значение). При этом $y$ - предполагаемый класс, а $x_i$ - значение признака у того объекта, который нужно классифицировать.\n",
    "\n",
    "По большому счету в нашей начальной формуле для вычисления вероятности того, что объект с данными признаками относится к конкретному классу, мы просто заменяем формулу вычисления вероятности как отношения количества соответствующих признаку объектов к общему числу объектов на данную, а дальше проводим идентичные вычисления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-aayUdS7qoK"
   },
   "source": [
    "Помимо гауссовского существуют еще два типа моделей на основе наивного байесовского классификатора: полиномиальная и с распределением Бернулли. *Полиномиальная* в основном используется для задачи классификации документов, т.е. определяет, к каким категориям относится текст: спорт, политика, техника и т.д. Используемые признаки являются частотой слов, присутствующих в документе. *Классификатор на основе распределения Бернулли* похож на полиномиальный метод, но признаками являются булевы переменные, т.е. они принимают только значения \"yes\" и \"no\" - например, встречается слово в тексте или нет. Используется в основном для классификации небольших текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kANvFfkY7qoL"
   },
   "source": [
    "Теперь импортируем модель GaussianNB из библиотеки sklearn и посмотрим, как она работает на уже известном нам датасете Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris_dataset = load_iris()\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris_dataset.data, iris_dataset.target, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F8JoKdjz7qoL"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EX5GmOxD7qoO"
   },
   "outputs": [],
   "source": [
    "nb_model = nb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATWmC0z_7qoS"
   },
   "source": [
    "Получим предсказания для тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ANWCBQTJ7qoT",
    "outputId": "6945cc63-287b-4f0e-8f3e-7cadb3756640"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 0, 1, 0, 2, 0, 0, 2, 2, 2, 1, 0,\n",
       "       2, 1, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_predictions = nb.predict(x_test)\n",
    "nb_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-XguYgt7qoV"
   },
   "source": [
    "Для определения точности предсказаний воспользуемся встроенной функцией *score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "a6d2F44w7qoW",
    "outputId": "80cfe26f-0abb-436e-c4b1-8b2d2489d728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy = nb.score(x_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shd16Vjg7qoY"
   },
   "source": [
    "Вспомним результаты метода kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_n = KNeighborsClassifier(n_neighbors=3)\n",
    "k_n_model = k_n.fit(x_train, y_train)\n",
    "knn_predictions = k_n_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ItshVsmh7qoZ",
    "outputId": "9b3794dd-a807-473f-ff5f-fd080aa44729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, knn_predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV7ledet7qoc"
   },
   "source": [
    "Как видно, на одних и тех же данных алгоритм GaussianNB работает так же, как и kNN. Вполне возможно, что при корректировке количества соседей в kNN метод будет работать точнее, но при равных условиях подбор оптимальных гиперпараметров займет явно больше времени, чем использование алгоритма \"из коробки\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz6fQE3t7qod"
   },
   "source": [
    "Несмотря на простоту, наивный байесовский алгоритм может быть удивительно точным. В целом, наивные байесовские алгоритмы используются в анализе настроений, фильтрации спама, системах рекомендаций и т.д. Они быстры и просты в реализации, но их основным недостатком является требование к признакам быть независимыми. В большинстве реальных датасетов они являются зависимыми, а это в свою очередь снижает производительность классификатора."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
