{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgshItiNbYmH"
   },
   "source": [
    "# Реализация табличной Q-функции\n",
    "\n",
    "В этом уроке мы поработаем с Q-функцией на практике. В данном уроке мы будем допускать, что Q-функция нам уже дана, и мы лишь учимся ею пользоваться для совершения оптимальных действий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1c9g1MHeUCl"
   },
   "source": [
    "### Загрузка библиотек\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2-zMowjHFz3t"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAGOjhbmg409"
   },
   "source": [
    "### Создание игровой среды\n",
    "Создадим симулятор Frozen Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "mIprlbAyJo18",
    "outputId": "fb9ddfcb-282b-4232-ac3d-6adf70f7ceda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 16\n",
      "Actions: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"ansi\")\n",
    "\n",
    "NUM_STATES = env.observation_space.n\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "\n",
    "print('States: {}'.format(NUM_STATES))\n",
    "print('Actions: {}'.format(NUM_ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtgwFse4AwVW"
   },
   "source": [
    "### Q-функция\n",
    "\n",
    "Создадим Q-функцию. По сути, это просто двумерная таблица размерности [количество состояний, количество действий].\n",
    "\n",
    "Пока что мы не знаем, как получить значения для Q-функции, так что заполним всю таблицу случайными значениями (просто для демо).\n",
    "\n",
    "Наша цель -- потренироваться в применении уже готовй Q-функции.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "LsxoLVvmZRx9",
    "outputId": "f83c4d4c-a31a-46d8-a100-201022c9ce3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3070543  0.38398111 0.1691199  0.05235285]\n",
      " [0.53005036 0.6787238  0.81498985 0.59939194]\n",
      " [0.22601009 0.19736971 0.52550929 0.50255182]\n",
      " [0.8659339  0.30286671 0.24161862 0.60970817]\n",
      " [0.58264057 0.27456743 0.74997804 0.11530879]\n",
      " [0.9908605  0.53374026 0.3231358  0.73676217]\n",
      " [0.65161068 0.03702514 0.88820115 0.59007946]\n",
      " [0.55818688 0.198568   0.76445408 0.0273465 ]\n",
      " [0.4367141  0.25007945 0.13156903 0.96220212]\n",
      " [0.29143295 0.90298395 0.01702414 0.08609321]\n",
      " [0.11782845 0.95929573 0.34769542 0.42296928]\n",
      " [0.88887226 0.92755255 0.38703767 0.9195419 ]\n",
      " [0.36879454 0.18141705 0.61783561 0.76022694]\n",
      " [0.20295096 0.62494804 0.97533967 0.48897309]\n",
      " [0.93318314 0.85002915 0.50185933 0.11869907]\n",
      " [0.07562305 0.75823667 0.7796322  0.72086581]]\n"
     ]
    }
   ],
   "source": [
    "Q = np.random.rand(NUM_STATES, NUM_ACTIONS)\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzGs5ElLB6Gk"
   },
   "source": [
    "### Запуск симуляции\n",
    "\n",
    "Запустим симуляцию для Frozen Lake так же, как мы делали до этого. Но только на этот раз будем использовать не случайную стратегию, а стратегию, основанную на Q-функции.\n",
    "\n",
    "Оптимальная политика (при условии оптимальности Q-функции) будет следующая: для текущего состояния `s` выбирать такое действие `a`, при котором значение `Q(s, a)` максимально.\n",
    "\n",
    "`a = np.argmax(Q[s,:])`\n",
    "\n",
    "В нашем случае Q-функция это просто какие-то случайные числа, поэтому агент ведет себя не очень оптимально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7ZQcB09FZfCC",
    "outputId": "c9aad494-ce11-4ad1-ddd3-38e327e44e19",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "\n",
      "3\n",
      "  (Up)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Final reward = 0.0\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()[0]\n",
    "\n",
    "for _ in range(100):\n",
    "    print(env.render())\n",
    "    a = np.argmax(Q[s,:]) # выбираем оптимальное действие\n",
    "    print(a)\n",
    "    s, r, done, trunc, _ = env.step(a)\n",
    "    if done:\n",
    "        print(env.render())\n",
    "        print('Final reward = {}'.format(r))\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI_rG4mmCwNS"
   },
   "source": [
    "**[Задание 1]** Заполните Q-функцию вручную такими значеними, чтобы агент (используя её для своей политики) доходил до цели и не падал в яму. Для этого задания значения в такой Q-функции не обязательно должны быть связыны с её формальным определением (через дисконтированную суммарную ганраду). Важно лишь то, чтобы политика `a = argmax Q(s, a)` приводила нас к цели. Проведите симуляцию с использованием этой Q-функции и посмотрите на результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.random.rand(NUM_STATES, NUM_ACTIONS)\n",
    "\n",
    "Q[:2, 2] = 1\n",
    "Q[2:11:4, 1] = 1\n",
    "Q[14, 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4502653 , 0.6081972 , 1.        , 0.99716233],\n",
       "       [0.8305342 , 0.64224576, 1.        , 0.42260257],\n",
       "       [0.36326036, 1.        , 0.62508432, 0.32600758],\n",
       "       [0.1635133 , 0.22165628, 0.12424581, 0.39045324],\n",
       "       [0.38263011, 0.06612429, 0.76806263, 0.69717439],\n",
       "       [0.29419004, 0.47117418, 0.41274833, 0.83808771],\n",
       "       [0.30937928, 1.        , 0.42771573, 0.7184081 ],\n",
       "       [0.33056922, 0.2939769 , 0.19827935, 0.75330334],\n",
       "       [0.51103903, 0.1401323 , 0.32003021, 0.40217055],\n",
       "       [0.80515014, 0.18073115, 0.75496105, 0.61399178],\n",
       "       [0.90442559, 1.        , 0.6002823 , 0.92456692],\n",
       "       [0.38019134, 0.07734396, 0.45315117, 0.18755549],\n",
       "       [0.2639159 , 0.52748805, 0.33311784, 0.09268708],\n",
       "       [0.63339712, 0.50105686, 0.95795025, 0.78582952],\n",
       "       [0.54707027, 0.47502346, 1.        , 0.39431792],\n",
       "       [0.97507458, 0.05229118, 0.83094057, 0.33483156]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "2\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "2\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Final reward = 1.0\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()[0]\n",
    "\n",
    "for _ in range(100):\n",
    "    print(env.render())\n",
    "    a = np.argmax(Q[s,:]) # выбираем оптимальное действие\n",
    "    print(a)\n",
    "    s, r, done, trunc, _ = env.step(a)\n",
    "    if done:\n",
    "        print(env.render())\n",
    "        print('Final reward = {}'.format(r))\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
