{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgshItiNbYmH"
   },
   "source": [
    "# Знакомство с Gym\n",
    "\n",
    "В этом уроке мы познакомимся с удобной средой для проведения RL экспериментов под названием Gym (https://gym.openai.com/). По аналогии с тем, как для обучения с учителем нам надо где-то брать обучающие датасеты, для обучения с подкреплением нам нужно иметь среду с определёнными правилами (или симуляцию это среды), в которой действует агент.\n",
    "\n",
    "в Gym содержатся реализации различных игр и других симуляций, на которых можно проводить эксперименты в области RL. Кроме самих симуляций в Gym имеется удобная обёртка для доступа к симуляции: мы сразу можем оперировтаь в терминах RL (состояние, действие, итд)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1c9g1MHeUCl"
   },
   "source": [
    "### Загрузка библиотек\n",
    "\n",
    "Загружаем библиотеку gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2-zMowjHFz3t"
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAGOjhbmg409"
   },
   "source": [
    "### Создание игровой среды\n",
    "\n",
    "С помощью функции `gym.make` мы можем создать симулятор необходимой нам игры. Рассмотрим пример игры `Frozen Lake`. Здесь есть ледяное поле и ямы (holes). Цель игры дойти до целевой позиции, не упав в яму. У этой игры есть парамтер `is_slippery`, который означает, \"будет ли лёд скользким\" -- будет ли среда всегда со стопроцентной вероятностью подчиняться нашему действию. Для простоты отключим этот флаг (куда захотели пойти, там и оказались).\n",
    "\n",
    "Другие доступные симуляции: https://gym.openai.com/envs/\n",
    "\n",
    "Посмотрим, сколько есть возможных действий и состояний в нашей игре. Кол-во состояний 16, так как столько ячеек в поле (потенциальных позиций для агента). А действий 4 (4 направления)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "mIprlbAyJo18",
    "outputId": "d3f8c2f1-a0d8-469a-9386-13c77b205315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 16\n",
      "Actions: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"ansi\")\n",
    "\n",
    "NUM_STATES = env.observation_space.n\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "\n",
    "print('States: {}'.format(NUM_STATES))\n",
    "print('Actions: {}'.format(NUM_ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJxgn6Uowdex"
   },
   "source": [
    "### Основные функции Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hI-7omyUwUqE"
   },
   "source": [
    "С помощью `env.reset()` можно перезапустить среду в исходное состояние (на начало эпизода). Эта функция также вернет начальное состояние.\n",
    "\n",
    "В нашем случае состояние это просто число -- индекс соответствующей ячейки, где находится робот."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OcUWhhTwHLyy",
    "outputId": "7e7f1fb0-cb1b-4a11-d43d-8d6474119c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'prob': 1})\n"
     ]
    }
   ],
   "source": [
    " s = env.reset()\n",
    " print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tfMM2sVw1fG"
   },
   "source": [
    "С помощью функции `env.render()` можно визуализировать текущее состояние среды. В Colab это не всегда можно сделать довольно просто (в случае сложных симуляций), но в случае Frozen Lake это просто напечатанный текст с нашим полем 4x4. S - start, F - frozen, H - hole, G - goal. Маркером указано положение робота."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "4LvlYZK7NMVo",
    "outputId": "16a95266-f9fe-406e-edb0-836f046c903b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnFEuExKxmie"
   },
   "source": [
    "Действия тоже кодируруются соответствующим индексом.\n",
    "Можно, например, выбрать случайное действие с попомщью функции `env.action_space.sample()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rxQjACvNHNul",
    "outputId": "27cc2461-d23c-4e63-b520-d524fb2433aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = env.action_space.sample()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vtt-jbKsxoWy"
   },
   "source": [
    "Чтобы совершить действие `a` нужно вызвать функцию `env.step(a)`. Эта функция вернет новое состояние (`s1`), в которое мы перешли, награду `r`, информацию о том, завершилась ли игра (`done`) и другую менее важную информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "dD-3K5-xHP8y",
    "outputId": "c49345e5-d3cc-42a9-dc10-42c29f20ea42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New state:  0\n",
      "Reward:  0.0\n",
      "Done?  False\n",
      "trunc False\n",
      "info {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "s1, r, done, trunc, info = env.step(a)\n",
    "\n",
    "print('New state: ', s1)\n",
    "print('Reward: ', r)\n",
    "print('Done? ', done)\n",
    "print(\"trunc\", trunc)\n",
    "print(\"info\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHjB31w2yVxY"
   },
   "source": [
    "Посмотрим, как теперь выглядит текущее состояние."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "7hn3ubSqNhDe",
    "outputId": "d709861e-608d-4b18-d93a-9f3df609d442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEJkMNdoycst"
   },
   "source": [
    "### Запуск симуляции\n",
    "\n",
    "Теперь у нас есть все знания, чтобы сыграть целый игровой эпизод с помощью Gym. Сначала встаём в стартовую позицию `env.reset()`. Потом в цикле совершаем шаги и рисуем промежуточные состояние с помощью `env.render()`. На каждом шаге нам как-то надо выбрать действие. Выбор действия обернём в функцию `a = policy(s)`. В идеале мы должны руководстсоваться некой стратегией, но в этом простом примере будем выбирать случайные действия `a`. После выбора действия `a` делаем шаг `env.step(a)` (сообщаем среде наше желание сделать действие). Если после определённого шага среда вернула `done=True`, значит произошел конец эпизода (упали в яму или дошли до цели). Если мы дошли до цели, то на последнем шаге мы должны были получить ненулевую награду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "id": "r82qS-BZHU5i",
    "outputId": "094cef8f-d0b8-4450-f31b-98971fcd4793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "1 2\n",
      "Reward = 0.0\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "2 2\n",
      "Reward = 0.0\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "6 1\n",
      "Reward = 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "10 1\n",
      "Reward = 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "\n",
      "14 1\n",
      "Reward = 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n",
      "15 2\n",
      "Reward = 1.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Final reward = 1.0\n"
     ]
    }
   ],
   "source": [
    "path_gen = (a for a in [2,2,1,1,1,2])\n",
    "\n",
    "\n",
    "\n",
    "def policy(s):\n",
    "    a = next(path_gen) # случайная стратегия\n",
    "    return a\n",
    "\n",
    "s = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    print(env.render())\n",
    "    a = policy(s)\n",
    "    s, r, done, trunc, _ = env.step(a)\n",
    "    print(s, a)\n",
    "    print('Reward = {}'.format(r))\n",
    "    if done:\n",
    "        print(env.render())\n",
    "        print('Final reward = {}'.format(r))\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3q1RuTqqzk9A"
   },
   "source": [
    "**[Задание 1]** Запрограммируйте бота вручную на совершение таких действий, при которых он дойдёт до цели и получит ненулевую награду. Для этого измените лишь функцию `policy()`. Просто создайте набор правил (стратегию) -- какое действие надо совершить в зависимости от состояния. Проведите симуляцию с использованием вашей стратегии и посмотрите на результат."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
