# ML educational progress
<details>
<summary>
    
## Beginner level ![](https://geps.dev/progress/65)
</summary>

- [x] Training with a teacher. Regression
- [x] Training with a teacher. Classification
- [x] Training with a teacher. Clustering
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/1_HW.ipynb)
---
- [x] ML formulation of the linear problem regression.
- [x] Advanced level of understanding linear regression.
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/linear_regression/1.1.ipynb)
---
- [x] Linear Regression Quality Metrics
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/linear_regression/3_7_%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0.ipynb)
---
- [x] Transformation of input data for linear regression
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/linear_regression/3_9_%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0.ipynb)
---
- [x] Polynomial regression
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/linear_regression/3_11_%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0.ipynb)
---
- [x] Regularization
- [x] Retraining using linear regression as an example
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/linear_regression/4_3_%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0.ipynb)
---
- [x] Mathematical magic regularization.
- [x] Training models using gradient descent
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/linear_regression/4_8_%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0.ipynb)
---
- [x] Mathematical magic of gradient descent
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/linear_regression/2.ipynb)
---
- [x] KNN algorithm
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/classification/homework_classification_1_les_3.ipynb)
---
- [x] Naive Bayes classifier
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/classification/homework_classification_1_les_6.ipynb)
---
- [x] Decision Trees Algorithm
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/classification/homework_classification_1_les_8.ipynb)
---
- [x] Classification quality metrics.
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/metrics/homework_classification_2_les_1_part_1.ipynb)
---
- [x] Multi-class classification
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/metrics/homework_classification_2_les_2.ipynb)
---
- [x] ML formulation of the clustering problem
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/clustering/jun_ml_7_hw_1.ipynb)
---
- [x] K-means algorithm
- [x] [Homework](https://github.com/hik023/data_science_training/blob/master/neural/clustering/jun_ml_7_hw_2.ipynb)
---
- [x] Selecting the number of clusters ùëò in the k-means algorithm
- [x] K-means algorithm in python
- [ ] DBSCAN Algorithm
- [ ] Homework
---
- [ ] Clustering quality metrics
- [ ] Homework
---
- [ ] Statement of the dimensionality reduction problem
- [ ] Using PCA for Dimensionality Reduction
- [ ] Homework
---
- [ ] Advanced: Implementing PCA Algorithm
- [ ] SVD conversion
- [ ] Homework
---
- [ ] t-SNE transformation
- [ ] Homework
---
- [ ] Introduction to Boosting
- [ ] Gradient Boosting and XGBoost in practice
- [ ] Homework
---
- [ ] Stacking
- [ ] Homework
---
- [ ] Why should you learn Kaggle?
- [ ] Working with Notebook and Kernel in Kaggle
- [ ] Homework

</details>
<details>
<summary>
    
## Advanced level (neural networks)
</summary>

will be filled after completing the previous level

</details>
<details>
<summary>

## Advanced level (recommendation systems)
</summary>

will be filled after completing the previous level
</details>